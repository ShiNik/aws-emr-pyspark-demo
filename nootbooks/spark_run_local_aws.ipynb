{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#              Imports                #\n",
    "#######################################\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType, LongType, DecimalType\n",
    "from functools import reduce\n",
    "import time\n",
    "from typing import AnyStr, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "access_key_id, secret_access_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#                Init                 #\n",
    "#######################################\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages=com.amazonaws:aws-java-sdk-bundle:1.12.134,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'\n",
    "conf = SparkConf().setAppName(\"s3tospark\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set('fs.s3a.access.key', access_key_id)\n",
    "hadoop_conf.set('fs.s3a.secret.key', secret_access_key)\n",
    "hadoop_conf.set('spark.hadoop.fs.s3a.aws.credintials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\\\n",
    "    .builder\\\n",
    "    .appName(\"godwit_data_extract\")\\\n",
    "    .master(\"local[8]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"s3a://emr-pyspark-2024/monthly_build/2024-05/input/test1.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(data_source)\n",
    "print(f\" Number of rows in SQL query: {df.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
